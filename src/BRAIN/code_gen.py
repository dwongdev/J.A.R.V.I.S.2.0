# import logging
# import os
# import pandas as pd
# import re
# from langchain_ollama import ChatOllama
# from src.FUNCTION.Tools.get_env import load_variable
# from src.FUNCTION.Tools.code_exec import execute_code_with_dependencies
# from google import genai

# # Configure logging (do this once at the beginning of your script)
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')
# logger = logging.getLogger(__name__)

# def provide_file_details(path: str) -> str:
#     logger.info(f"Providing details for file: {path}")
#     try:
#         df = pd.read_csv(path)
#         details = [
#             f"File Path: {path}",
#             f"File Size: {df.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB",
#             f"Shape (rows, columns): {df.shape}",
#             f"Column Names: {df.columns.tolist()}",
#             f"Data Types:\n{df.dtypes.to_string()}",
#             f"First 5 rows:\n{df.head().to_string(index=False)}",
#             f"Missing values:\n{df.isnull().sum().to_string()}",
#             f"Summary statistics:\n{df.describe(include='all').to_string()}",
#             f"Unique values per column:\n{df.nunique().to_string()}",
#             f"Sample value types per column:\n{df.iloc[0].to_dict()}"
#         ]
#         return "\n\n".join(details)
#     except FileNotFoundError:
#         logger.error(f"File not found at path: {path}")
#         return ""
#     except Exception as e:
#         logger.error(f"Error while processing file {path}: {e}", exc_info=True) # Log with traceback
#         return ""

# def extract_python_code(text):
#     pattern = r"```python\s*(.*?)\s*```"
#     match = re.search(pattern, text, re.DOTALL)
#     if match:
#         code = match.group(1).strip()
#         logger.debug(f"Extracted Python code:\n{code}")
#         return code
#     else:
#         logger.info("No Python code found in the text.")
#         return ""

# def generate_refactor_prompt(code: str, error: str, file_description: str):
#     """
#     Generate a comprehensive prompt for a code refactoring assistant to analyze the code and refactor it.

#     Args:
#         code (str): The Python code that generated the error.
#         error (str): The error message generated by the code.
#         file_description (str): The detailed description of the file being used in the code.

#     Returns:
#         str: A detailed prompt for refactoring the code.
#     """
#     full_prompt = f"""
#     You are an expert Python code refactor assistant. The user has provided code that generated an error during execution.
#     The task is to analyze the code and the accompanying error, then refactor the code to resolve the issue.
#     The code uses standard Python libraries such as pandas, numpy, and matplotlib, and works with the following file.

#     File Description:
#     {file_description}

#     The Python code provided is as follows:

#     Code:
#     {code}

#     The error generated is:

#     Error:
#     {error}

#     Instructions:
#     - Thoroughly examine the provided code and understand its logic.
#     - Investigate the error message and identify the root cause.
#     - Refactor the code to fix the error, making it syntactically correct and functional.
#     - Ensure that the code properly utilizes the libraries (pandas, numpy, matplotlib) and their functionalities.
#     - If the error relates to file handling (e.g., file not found or data loading issues), ensure the code correctly handles file paths and errors.
#     - Check for common pitfalls such as incorrect file paths, missing libraries, or improperly used functions.
#     - After refactoring, ensure that the code works as expected and generates the correct output without errors.

#     Your goal is to refine the code to make it error-free, maintainable, and functional. Pay special attention to the correct use of libraries and error handling.
#     """
#     logger.debug(f"Generated refactor prompt:\n{full_prompt}")
#     return full_prompt

# def gem_refactor_code(first_response_code: str, file_path: str, attempt: int = 1, max_attempts: int = 3):
#     logger.info(f"Attempting to refactor code (attempt {attempt}/{max_attempts})")
#     if not first_response_code:
#         logger.warning("No code provided for refactoring.")
#         return {"error": "No code generated."}

#     exec_info = execute_code_with_dependencies(first_response_code)
#     logger.info(f"Code execution result (attempt {attempt}): {exec_info}")
    
#     if exec_info.get("error"):
#         ERROR = exec_info.get("error")
#         logger.error(f"Code execution error (attempt {attempt}): {ERROR}")
#         if attempt >= max_attempts:
#             logger.error(f"Max refactor attempts reached. Last error: {ERROR}")
#             return {"error": f"Max attempts reached. Last error: {ERROR}"}

#         file_description = provide_file_details(file_path)
#         full_prompt = generate_refactor_prompt(first_response_code, ERROR, file_description)

#         api_key = load_variable("genai_key")
#         try:
#             client = genai.Client(api_key=api_key)
#             logger.info("Calling Gemini API for code refactoring.")
#             response_code = client.models.generate_content(
#                 model='gemini-2.0-flash',
#                 contents=full_prompt
#             )
#             refactored_code = extract_python_code(response_code.text)
#             logger.info(f"Received refactored code from Gemini (attempt {attempt}):\n{refactored_code}")
#             return gem_refactor_code(refactored_code, file_path, attempt + 1, max_attempts)
#         except Exception as e:
#             logger.error(f"Error during Gemini API call (attempt {attempt}): {e}", exc_info=True)
#             return {"error": f"Error during Gemini API call: {e}"}

#     logger.info(f"Code executed successfully after {attempt} attempt(s).")
#     return exec_info

# def gem_text_to_code(user_prompt: str, file_path: str):
#     logger.info(f"Generating code from user prompt using Gemini: '{user_prompt}'")
#     api_key = load_variable("genai_key")
#     client = genai.Client(api_key=api_key)

#     if not os.path.exists(file_path):
#         logger.error(f"CSV file not found at path: {file_path}")
#         raise FileNotFoundError(f"{file_path} not found!.")

#     file_description = provide_file_details(file_path)
#     full_prompt = f"""
#     You are a Python data analysis assistant. The user has provided a CSV file at path: '{file_path}'.

#     Use standard Python libraries like pandas, numpy, and matplotlib where needed.
#     Use the following information about the file:
#     {file_description}

#     User Query: {user_prompt}

#     Generate the correct code using the CSV path and context. Make sure itâ€™s syntactically valid and relevant.
#     """
#     logger.debug(f"Generated Gemini prompt for text-to-code:\n{full_prompt}")

#     try:
#         logger.info("Calling Gemini API for text-to-code.")
#         response_code = client.models.generate_content(
#             model='gemini-2.0-flash',
#             contents=full_prompt
#         )
#         generated_code = extract_python_code(response_code.text)
#         if generated_code:
#             logger.info("Generated Python code from Gemini.")
#             exec_info = gem_refactor_code(generated_code, file_path)
#             if exec_info.get("output"):
#                 return exec_info.get("output")
#             return exec_info.get("error")
#         else:
#             logger.warning("Gemini did not generate any executable Python code.")
#             return None
#     except Exception as e:
#         logger.error(f"Error during Gemini API call for text-to-code: {e}", exc_info=True)
#         return None

# def local_refactor_code(first_response_code: str, file_path: str, attempt: int = 1, max_attempts: int = 10):
#     logger.info(f"Attempting to locally refactor code (attempt {attempt}/{max_attempts})")
#     if not first_response_code:
#         logger.warning("No code provided for local refactoring.")
#         return {"error": "No code generated."}

#     exec_info = execute_code_with_dependencies(first_response_code)
#     logger.info(f"Local code execution result (attempt {attempt}): {exec_info}")

#     if exec_info.get('error'):
#         ERROR = exec_info.get("error")
#         logger.error(f"Local code execution error (attempt {attempt}): {ERROR}")
#         if attempt >= max_attempts:
#             logger.error(f"Max local refactor attempts reached. Last error: {ERROR}")
#             return {"error": f"Max attempts reached. Last error: {ERROR}"}

#         file_description = provide_file_details(file_path)
#         full_prompt = generate_refactor_prompt(first_response_code, ERROR, file_description)

#         model = load_variable("Text_to_info_model")
#         try:
#             llm = ChatOllama(
#                 model=model,
#                 temperature=0.3,
#             )
#             messages = [
#                 {"role": "system", "content": full_prompt},
#                 {"role": "user", "content": first_response_code}
#             ]
#             logger.info("Calling local LLM (Ollama) for code refactoring.")
#             response_code = llm.invoke(messages)
#             refactored_code = extract_python_code(response_code)
#             logger.info(f"Received refactored code from local LLM (attempt {attempt}):\n{refactored_code}")
#             return local_refactor_code(refactored_code, file_path, attempt + 1, max_attempts)
#         except Exception as e:
#             logger.error(f"Error during local LLM invocation (attempt {attempt}): {e}", exc_info=True)
#             return {"error": f"Error during LLM invocation: {str(e)}"}

#     logger.info(f"Local code executed successfully after {attempt} attempt(s).")
#     return exec_info

# def local_text_code(user_prompt: str, file_path: str):
#     logger.info(f"Generating code from user prompt using local LLM: '{user_prompt}'")
#     model = load_variable("Text_to_info_model")

#     file_description = provide_file_details(file_path)
#     full_prompt = f"""
#     You are a Python data analysis assistant. The user has provided a CSV file at path: '{file_path}'.

#     Use standard Python libraries like pandas, numpy, and matplotlib where needed.
#     Use the following information about the file:
#     {file_description}

#     Generate the correct code using the CSV path and context. Make sure itâ€™s syntactically valid and relevant.
#     """
#     logger.debug(f"Generated local LLM prompt for text-to-code:\n{full_prompt}")

#     try:
#         llm = ChatOllama(
#             model=model,
#             temperature=0.3,
#         )
#         messages = [
#             {"role": "system", "content": full_prompt},
#             {"role": "user", "content": user_prompt}
#         ]
#         logger.info("Calling local LLM (Ollama) for text-to-code.")
#         response_code = llm.invoke(messages)
#         generated_code = extract_python_code(response_code)
#         if generated_code:
#             logger.info("Generated Python code from local LLM.")
#             exec_info = local_refactor_code(generated_code, file_path)
#             if exec_info.get("output"):
#                 return exec_info.get("output")
#             return exec_info.get("error")
#         else:
#             logger.warning("Local LLM did not generate any executable Python code.")
#             return None
#     except Exception as e:
#         logger.error(f"Error during local LLM invocation for text-to-code: {e}", exc_info=True)
#         return None
    

# def data_analysis(user_prompt: str , file_path:str):
#     try:
#         # Try to generate code via API
#         response = gem_text_to_code(user_prompt , file_path)
        
#     except Exception as e:
#         # If API fails, log the error and fall back to local code generation
#         logger.error(f"ERROR: {e} - Falling back to local processing.")
#         response = local_text_code(user_prompt , file_path)
#     return response


# ....

import logging
import os
import pandas as pd
import re
from langchain_ollama import ChatOllama
from src.FUNCTION.Tools.get_env import EnvManager
from src.FUNCTION.Tools.code_exec import CodeExecutor
from google import genai

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')
logger = logging.getLogger(__name__)


class CodeRefactorAssistant:
    def __init__(self):
        self.genai_key = EnvManager.load_variable("genai_key")
        self.local_model = EnvManager.load_variable("Text_to_info_model")
        self.execute_code_with_dependencies = CodeExecutor()
    def provide_file_details(self, path: str) -> str:
        logger.info(f"Providing details for file: {path}")
        try:
            df = pd.read_csv(path)
            details = [
                f"File Path: {path}",
                f"File Size: {df.memory_usage(deep=True).sum() / (1024 ** 2):.2f} MB",
                f"Shape (rows, columns): {df.shape}",
                f"Column Names: {df.columns.tolist()}",
                f"Data Types:\n{df.dtypes.to_string()}",
                f"First 5 rows:\n{df.head().to_string(index=False)}",
                f"Missing values:\n{df.isnull().sum().to_string()}",
                f"Summary statistics:\n{df.describe(include='all').to_string()}",
                f"Unique values per column:\n{df.nunique().to_string()}",
                f"Sample value types per column:\n{df.iloc[0].to_dict()}"
            ]
            return "\n\n".join(details)
        except FileNotFoundError:
            logger.error(f"File not found at path: {path}")
            return ""
        except Exception as e:
            logger.error(f"Error while processing file {path}: {e}", exc_info=True)
            return ""

    def extract_python_code(self, text):
        pattern = r"```python\s*(.*?)\s*```"
        match = re.search(pattern, text, re.DOTALL)
        if match:
            return match.group(1).strip()
        logger.info("No Python code found in the text.")
        return ""

    def generate_refactor_prompt(self, code: str, error: str, file_description: str):
        return f"""
        You are an expert Python code refactor assistant. The user has provided code that generated an error during execution.

        File Description:
        {file_description}

        Code:
        {code}

        Error:
        {error}

        Instructions:
        - Analyze and refactor to resolve the issue.
        - Use pandas, numpy, matplotlib correctly.
        - Handle file-related errors.
        - Refactor to be functional and error-free."""

    def gem_refactor_code(self, code: str, file_path: str, attempt=1, max_attempts=3):
        logger.info(f"Gemini refactor attempt {attempt}/{max_attempts}")
        if not code:
            return {"error": "No code generated."}

        exec_info = self.execute_code_with_dependencies.execute_code(code)
        if exec_info.get("error"):
            if attempt >= max_attempts:
                return {"error": f"Max attempts reached. Last error: {exec_info['error']}"}

            file_desc = self.provide_file_details(file_path)
            prompt = self.generate_refactor_prompt(code, exec_info["error"], file_desc)
            try:
                client = genai.Client(api_key=self.genai_key)
                response = client.models.generate_content(model='gemini-2.0-flash', contents=prompt)
                refactored = self.extract_python_code(response.text)
                return self.gem_refactor_code(refactored, file_path, attempt + 1)
            except Exception as e:
                logger.error("Gemini API failed", exc_info=True)
                return {"error": str(e)}

        return exec_info

    def gem_text_to_code(self, user_prompt: str, file_path: str):
        logger.info("Generating code from Gemini")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"{file_path} not found!")

        file_desc = self.provide_file_details(file_path)
        prompt = f"""
            You are a Python data analysis assistant. The user has provided a CSV file at path: '{file_path}'.

            Use pandas, numpy, matplotlib.
            File Description:
            {file_desc}

            User Query:
            {user_prompt}"""
        try:
            client = genai.Client(api_key=self.genai_key)
            response = client.models.generate_content(model='gemini-2.0-flash', contents=prompt)
            code = self.extract_python_code(response.text)
            return self.gem_refactor_code(code, file_path)
        except Exception as e:
            logger.error("Gemini API failed", exc_info=True)
            return {"error": str(e)}

    def local_refactor_code(self, code: str, file_path: str, attempt=1, max_attempts=10):
        logger.info(f"Local refactor attempt {attempt}/{max_attempts}")
        if not code:
            return {"error": "No code generated."}

        exec_info = self.execute_code_with_dependencies.execute_code(code)
        if exec_info.get("error"):
            if attempt >= max_attempts:
                return {"error": f"Max attempts reached. Last error: {exec_info['error']}"}

            file_desc = self.provide_file_details(file_path)
            prompt = self.generate_refactor_prompt(code, exec_info["error"], file_desc)
            try:
                llm = ChatOllama(model=self.local_model, temperature=0.3)
                messages = [
                    {"role": "system", "content": prompt},
                    {"role": "user", "content": code}
                ]
                response = llm.invoke(messages)
                refactored = self.extract_python_code(response)
                return self.local_refactor_code(refactored, file_path, attempt + 1)
            except Exception as e:
                logger.error("Local LLM failed", exc_info=True)
                return {"error": str(e)}

        return exec_info

    def local_text_to_code(self, user_prompt: str, file_path: str):
        logger.info("Generating code from local LLM")
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"{file_path} not found!")

        file_desc = self.provide_file_details(file_path)
        prompt = f"""
            You are a Python data analysis assistant. The user has provided a CSV file at path: '{file_path}'.

            Use pandas, numpy, matplotlib.
            File Description:
            {file_desc}

            User Query:
            {user_prompt}"""
        try:
            llm = ChatOllama(model=self.local_model, temperature=0.3)
            messages = [
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_prompt}
            ]
            response = llm.invoke(messages)
            code = self.extract_python_code(response)
            return self.local_refactor_code(code, file_path)
        except Exception as e:
            logger.error("Local LLM failed", exc_info=True)
            return {"error": str(e)}


def data_analysis(user_prompt: str , file_path:str):
    codeassistant = CodeRefactorAssistant()
    try:
        # Try to generate code via API
        response = codeassistant.gem_text_to_code(user_prompt , file_path)
        
    except Exception as e:
        # If API fails, log the error and fall back to local code generation
        logger.error(f"ERROR: {e} - Falling back to local processing.")
        response = codeassistant.local_text_to_code(user_prompt , file_path)

    return response

